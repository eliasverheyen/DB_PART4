{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werkje gegevensbanken 2018 - DEEL 04\n",
    "\n",
    "## Inleiding\n",
    "\n",
    "Het vierde en laatste deel van het werkje bestaat uit twee componenten: \n",
    "1. Optimalisatie\n",
    "2. Visualisatie\n",
    "\n",
    "Bij dit deel van het werkje laten we jullie veel vrijheid om tot een oplossing te komen. Er bestaat niet zoiets als de unieke correcte oplossing, het is dus de bedoeling dat jullie zelf een strategie uitdokteren en deze achteraf kunnen verdedigen. Leg in deze notebook kort uit welke keuzes je maakt en waarom, dit zal helpen bij de verdediging. Beschouw dit als een soort mini-verslag tussen de code door.\n",
    "\n",
    "### Indienen + Evaluatiemoment\n",
    "\n",
    "** EVALULATIEMOMENT**: DE OEFENZITTING IN DE WEEK VAN 14/05/2017-18/05/2017\n",
    "\n",
    "Het evaluatiemoment van het **volledige** werkje vindt plaats tijdens **de oefenzitting in de week van 14/05/2017-18/05/2017**. Prof. Berendt en dr. Bogaerts komen langs om enkele vragen te stellen over jullie oplossingen. Dit evaluatiemoment omvat ook een demonstratie van jullie resultaat, bijvoorbeeld: aantonen dat jullie optimalisatie van deel 4 van het werkje effectief een verbetering is t.o.v. het originele geval. Teneinde die demonstratie vlot te doen verlopen vragen we jullie om **per groep (minstens) 1 laptop mee te nemen waarop jullie oplossingen (i.e. de ingevulde notebooks) goed functioneren**. Deze laptop beschikt dus ook over XAMPP (inclusief de gegevensbank van dit werkje, uiteraard), gezien de queries in de notebooks ook moeten werken.\n",
    "\n",
    "**INDIENDEADLINE**: ZONDAG 13 MEI om 23.59\n",
    "\n",
    "** HOE INDIENEN**:\n",
    "    1. Upload je ingevulde notebook als werkje_04_groep_XX.ipynb EN werkje_04_groep_XX.html \n",
    "    2. Vervang XX door je groepsnummer.\n",
    "    3. Uploaden doe je op Toledo in je groepsfolder.\n",
    "    \n",
    "Zorg ervoor dat de .html file zeker ook de output van de visualisatie bevat, zodat wij deze gemakkelijk kunnen bekijken en niet moeten reproduceren door jullie .ipynb te runnen! Op die manier kunnen we zeker zijn dat we evalueren wat jullie geproduceerd hebben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuttige packages en functies inladen\n",
    "\n",
    "Enkele nuttige packages en functies inladen die verderop van pas zullen komen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benodigde packages\n",
    "import json            # Package om .json files in te laden (bvb kolomnamen zijn zo opgeslagen)\n",
    "import getpass         # Package om een paswoordveldje te genereren.\n",
    "import mysql.connector # MySQL package\n",
    "import numpy as np\n",
    "import pandas as pd    # Populaire package voor data-verwerking\n",
    "import sys\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version_info       # Check python versie, wij veronderstellen 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interageren met een gegevensbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbind_met_GB(username, hostname, gegevensbanknaam):\n",
    "    \"\"\"\n",
    "    Maak verbinding met een externe gegevensbank\n",
    "    \n",
    "    :param  username:          username van de gebruiker, string\n",
    "    :param  hostname:          naam van de host, string.\n",
    "                               Dit is in het geval van een lokale server gewoon 'localhost'\n",
    "    :param  gegevensbanknaam:  naam van de gegevensbank, string.\n",
    "    :return connection:        connection object, dit is wat teruggeven wordt \n",
    "                               door connect() methods van packages die voldoen aan de DB-API\n",
    "    \"\"\"\n",
    "    \n",
    "    password = getpass.getpass() # Genereer vakje voor wachtwoord in te geven\n",
    "    \n",
    "    connection = mysql.connector.connect(host=hostname,\n",
    "                                         user=username,\n",
    "                                         passwd=password,\n",
    "                                         db=gegevensbanknaam)\n",
    "    return connection\n",
    "\n",
    "\n",
    "def check_perfect_match(df1, df2):\n",
    "    \"\"\"\n",
    "    Functie om te checken of 2 DataFrames gelijk zijn.\n",
    "    \"\"\"\n",
    "    check = df1.equals(df2)\n",
    "    return check\n",
    "\n",
    "\n",
    "def run_query(connection, query):\n",
    "    \"\"\"\n",
    "    Voer een query uit op een reeds gemaakte connectie, geeft het resultaat van de query terug\n",
    "    \"\"\"\n",
    "    \n",
    "    # Making a cursor and executing the query\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    # Collecting the result and casting it in a pd.DataFrame\n",
    "    res = cursor.fetchall()\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def res_to_df(query_result, column_names):\n",
    "    \"\"\"\n",
    "    Giet het resultaat van een uitgevoerde query in een 'pandas dataframe'\n",
    "    met vooraf gespecifieerde kolomnamen.\n",
    "    \n",
    "    Let op: Het resultaat van de query moet dus exact evenveel kolommen bevatten\n",
    "    als kolomnamen die je meegeeft. Als dit niet het geval is, is dit een indicatie\n",
    "    dat je oplossing fout is. (Gezien wij de kolomnamen van de oplossing al cadeau doen)\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(query_result, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalisatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "Het doel van het eerste deel van deze taak is het optimaliseren van onderstaande query.\n",
    "\n",
    "Een goede oplossing van deze eerste opgave voldoet aan volgende criteria:\n",
    "* Verzin een optimalisatie van deze query\n",
    "* De runtime van de optimalisatie is significant minder dan die van de originele oplossing (een paar procent is te weinig)\n",
    "* Je bent in staat om uit te leggen WAT je doet en WAAROM je dat gedaan hebt en WAAROM het werkt..\n",
    "    * Het EXPLAIN statement in MySQL is een goede gids om je hierbij te helpen. \n",
    "    * Je schrijft in deze notebook al kort de redeneringen op die achter jullie optimalisatie zitten (dit zal nuttig zijn bij de mondelinge verdediging).\n",
    "\n",
    "Hieronder vinden jullie de query die te optimaliseren is. Wijzig deze dus niet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beschrijving**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het resultaat van deze functie is een Pandas DataFrame dat voor een gegeven *jaar_1* een aantal statistieken van alle staten bevat waarbij de gemiddelde lengte van alle spelers geboren in die staat en opgenomen in de hall of fame na *jaar_2* groter is dan *lengte*.\n",
    "\n",
    "Voor die staten moet de tabel de volgende statistieken bevatten: het gemiddelde gewicht, de gemiddelde lengte, het gemiddeld aantal batting homeruns, en het gemiddeld aantal pitching saves van alle spelers (geboren in die staat) die in de hall of fame zijn opgenomen na *jaar_2*.\n",
    "\n",
    "Sorteer oplopend alfabetisch op staat.\n",
    "\n",
    "Nb. Lengte is uitgedrukt in inches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['state', 'avg_weight', 'avg_height', 'avg_homeruns', 'avg_saves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dit is de originele query die geoptimaliseerd dient te worden. Niet wijzigen!\n",
    "def query_to_optimize(connection, column_names, jaar_1=2000, jaar_2=1990, lengte=75):\n",
    "    # Actual query\n",
    "    query=\"\"\"\n",
    "    SELECT m.birthState, AVG(m.weight), AVG(m.height), AVG(bat.HR), AVG(pit.SV)\n",
    "    FROM Master AS m,\n",
    "        Pitching AS pit,\n",
    "        Batting AS bat,\n",
    "        HallOfFame AS hof\n",
    "    WHERE pit.yearID = {}\n",
    "        AND bat.yearID = {}\n",
    "        AND pit.playerID = m.playerID\n",
    "        AND bat.playerID = m.playerID\n",
    "        AND m.playerID = hof.playerID\n",
    "        AND hof.yearID > {}\n",
    "    GROUP BY m.birthState\n",
    "    HAVING AVG(m.height) > {}\n",
    "    ORDER BY m.birthState ASC;\n",
    "    \"\"\".format(jaar_1, jaar_1, jaar_2, lengte)\n",
    "    \n",
    "    # Stap 2 & 3\n",
    "    res = run_query(connection, query)         # Query uitvoeren\n",
    "    df = res_to_df(res, column_names)          # Query in DataFrame brengen\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eerst maken we verbinding met de gegevensbank, zoals jullie al deden in het vorige deel van het werkje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'root'      # Vervang dit als je via een andere user queries stuurt\n",
    "hostname = 'localhost' # Als je een databank lokaal draait, is dit localhost.\n",
    "db = 'lahman2016'      # Naam van de gegevensbank op je XAMPP Mysql server\n",
    "\n",
    "# We verbinden met de gegevensbank\n",
    "c = verbind_met_GB(username, hostname, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In onderstaande cel bekomen we de runtime van de originele query door deze vijf keer te runnen, om tot een betere schatting te komen. Deze conventie van runtimes meten dient behouden te worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings=[]\n",
    "\n",
    "# We voeren deze query 5 keer uit om een betere schatting te krijgen van de effectieve runtime.\n",
    "for i in range(5):\n",
    "    t1 = time.time() # Start time\n",
    "    df_orig = query_to_optimize(c, column_names, jaar_1=2000, jaar_2=1990, lengte=75)\n",
    "    t2 = time.time() # Stop time\n",
    "    timings.append(t2-t1) \n",
    "    \n",
    "orig_runtime=np.mean(timings) # De uiteindelijke runtime is een gemiddelde van 5 runs.\n",
    "\n",
    "print('De originele query duurt: ', \"{0:.2f}\".format(orig_runtime),' seconden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimalisatie\n",
    "\n",
    "In deze sectie is het de bedoeling dat je je optimalisatie realiseert. Wat hier al ingevuld staat qua code is enkel om je op weg te helpen.\n",
    "\n",
    "Je mag zoveel codecellen toevoegen als je nodig acht om je verhaal te vertellen. Leg uit wat je doet, en waarom je dat doet. Denk hierbij zeker aan het EXPLAIN statement zoals gezien in de les. Gebruik EXPLAIN om te identificeren hoe MySQL de query omzet en kijk zo welke effecten je verbeteringen hebben. Leg de resultaten van EXPLAIN kort uit.\n",
    "\n",
    "Beschouw de uitleg rond je oplossing als een soort mini-verslag dat je code en query uitlegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak hier je eerste optimalisatie\n",
    "def optim_1(connection, column_names, jaar_1=2000, jaar_2=1990, lengte=75):\n",
    "    query=\"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stap 2 & 3\n",
    "    res = run_query(connection, query)         # Query uitvoeren\n",
    "    df = res_to_df(res, column_names)          # Query in DataFrame brengen\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controle\n",
    "\n",
    "De onderstaande codecel vergelijkt de runtime van de optimalisatie met die van de originele query, door een gemiddelde te nemen over 5 runs. Ook wordt er nagegaan dat de resultaten zeker volledig gelijk zijn. Wijzig deze controlewijze niet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hier je eerste optimalisatie\n",
    "\n",
    "timings_optim_1 = []\n",
    "for i in range(5):\n",
    "    t1 = time.time() # Start time\n",
    "    df_optim_1 = optim_1(c, column_names, jaar_1=2000, jaar_2=1990, lengte=75)\n",
    "    t2 = time.time() # Stop time\n",
    "    timings_optim_1.append(t2-t1) \n",
    "    \n",
    "optim_1_runtime = np.mean(timings_optim_1) # Runtime optimalisatie 1\n",
    "\n",
    "# Vergelijken met originele query\n",
    "diff = orig_runtime-optim_1_runtime # Winst t.o.v. origineel\n",
    "rel_diff = diff/orig_runtime # Relatieve winst\n",
    "check = check_perfect_match(df_orig,df_optim_1) # Nagaan of de resultaten exact gelijk zijn.\n",
    "\n",
    "# Belangrijkste resultaten printen.\n",
    "print('De optimalisatie duurt: ', \"{0:.2f}\".format(optim_1_runtime),' seconden')\n",
    "print('De originele versie duurde: ', \"{0:.2f}\".format(orig_runtime),' seconden')\n",
    "print('De netto tijdwinst is dus: ',\"{0:.2f}\".format(diff),' seconden, oftewel', \"{0:.2f}\".format(100*rel_diff), '%' )\n",
    "print('Is het resultaat equivalent? ', check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisatie\n",
    "\n",
    "### Opgave\n",
    "\n",
    "In het laatste onderdeel van het werkje laten we jullie volledig vrij. Jullie hebben ondertussen geleerd om data uit een gegevensbank op te halen, de volgende stap is om deze data effectief voor iets nuttigs te gebruiken.\n",
    "\n",
    "Een snelle manier om gegevens overzichtelijk weer te geven is visualisatie. Een goede visualisatie kan zeker in een eerste fase van data-analyse echt al inzichten verschaffen. Wij vragen dus aan jullie een of meerdere visualisaties te realiseren, met data uit de gegevensbank die jullie voor dit werkje gebruikt hebben.\n",
    "\n",
    "Een goede oplossing van deze opgave voldoet aan volgende criteria:\n",
    "* Schrijf een query die informatie uit de gegevensbank ophaalt, en in een DataFrame stopt.\n",
    "* Maak een interessante visualisatie van (een deel van) de data in die DataFrame\n",
    "* Je beschrijft kort (mini-verslag hier in de notebook) wat je doet, en wat de inzichten zijn die de visualisatie verschaft.\n",
    "* Je bent in staat om dit mini-verslag ook mondeling te verdedigen, i.e. antwoorden op: wat doe je, waarom doe je dit, en wat was het resultaat.\n",
    "* 1 visualisatie volstaat, meer mag altijd.\n",
    "\n",
    "### Nuttige packages\n",
    "\n",
    "Om jullie al een beetje op weg te helpen, geven we jullie alvast de naam van twee handige en veelvoorkomende visualisatietools die goed werken met pandas DataFrames:\n",
    "\n",
    "1. seaborn\n",
    "2. matplotlib\n",
    "\n",
    "Voor een korte inleiding ivm seaborn kan je hier kijken: https://chrisalbon.com/python/pandas_with_seaborn.html\n",
    "Voor een korte inleiding ivm matplotlib kan je hier kijken: http://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "Natuurlijk moet je vooraleer je aan zo'n visualisatie kan beginnen, al een pandas DataFrame hebben dat de juiste informatie bevat. De eerste stap zal er dus in bestaan om een goede query te schrijven die de nodige informatie uit de gegevensbank weet te halen, gevolgd door een visualisatie van de DataFrame die uit die query volgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALISATIE 1\n",
    "column_names2 = ['weight', 'avg_salary']\n",
    "\n",
    "def vis_query(connection, column_names):\n",
    "    query=\"\"\"\n",
    "    SELECT m.Weight, AVG(s.Salary)\n",
    "    FROM salaries as s, Master as m\n",
    "    WHERE m.PlayerID = s.PlayerID\n",
    "    GROUP BY(m.PlayerID)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stap 2 & 3\n",
    "    res = run_query(connection, query)         # Query uitvoeren\n",
    "    df = res_to_df(res, column_names)          # Query in DataFrame brengen\n",
    "    \n",
    "    return df\n",
    "\n",
    "vis_pandas = vis_query(c, column_names2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "vis_pandas['avg_salary'] = vis_pandas['avg_salary'].astype('float')\n",
    "vis_pandas.dtypes\n",
    "vis_pandas.plot.scatter(x='avg_salary', y='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "vis_np = np.array(vis_pandas)\n",
    "#numpy array verliest kolomnaam, 1ste is avg_salary, 2de is weight\n",
    "plt.scatter(vis_np[:, 0], vis_np[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster as cl\n",
    "\n",
    "for i in range(2, 10, 2):\n",
    "    plt.figure(tight_layout=True)\n",
    "    \n",
    "    plt.subplot(141)\n",
    "    plt.yscale('log')\n",
    "    clf = cl.AgglomerativeClustering(i)\n",
    "    y = clf.fit_predict(vis_np)\n",
    "    plt.title(\"Ward-clustering: %d\" % i)\n",
    "    plt.scatter(vis_np[:, 0], vis_np[:, 1], c=y, cmap='viridis')\n",
    "\n",
    "    plt.subplot(142)\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"KMeans-clustering: %d\" % i)\n",
    "    clf = cl.KMeans(i)\n",
    "    y = clf.fit_predict(vis_np)\n",
    "    plt.scatter(vis_np[:, 0], vis_np[:, 1], c=y, cmap='viridis')\n",
    "    \n",
    "    plt.subplot(143)\n",
    "    plt.yscale('log')\n",
    "    clf = cl.AgglomerativeClustering(i+1)\n",
    "    y = clf.fit_predict(vis_np)\n",
    "    plt.title(\"Ward-clustering: %d\" % (i+1))\n",
    "    plt.scatter(vis_np[:, 0], vis_np[:, 1], c=y, cmap='viridis')\n",
    "\n",
    "    plt.subplot(144)\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"KMeans-clustering: %d\" % (i+1))\n",
    "    clf = cl.KMeans(i+1)\n",
    "    y = clf.fit_predict(vis_np)\n",
    "    plt.scatter(vis_np[:, 0], vis_np[:, 1], c=y, cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(tight_layout=True)\n",
    "plt.yscale('log')\n",
    "bandwidth = cl.estimate_bandwidth(vis_np)\n",
    "clf = cl.MeanShift(bin_seeding=True)\n",
    "y = clf.fit_predict(vis_np)\n",
    "plt.title(\"MeanShift-clustering\")\n",
    "plt.scatter(vis_np[:, 0], vis_np[:, 1], c=y, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2de visualisatie:\n",
    "#### avg homeruns vs avg salaris\n",
    "Merk op dat tupels met bijna 0 avg homeruns niet meegenomen worden en merk op dat avg homeruns * 10 gedaan wordt, arbitrair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALISATIE 2\n",
    "column_names3 = ['avg_homeruns', 'avg_salary']\n",
    "\n",
    "def vis_query(connection, column_names):\n",
    "    query=\"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "    SELECT AVG(b.HR)*10 as ah, AVG(s.salary)\n",
    "    FROM salaries as s, Master as m, battingpost as b\n",
    "    WHERE m.PlayerID = s.PlayerID AND m.playerID = b.playerID\n",
    "    GROUP BY(m.PlayerID)\n",
    ") as sub\n",
    "    WHERE sub.ah>0.01\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stap 2 & 3\n",
    "    res = run_query(connection, query)         # Query uitvoeren\n",
    "    df = res_to_df(res, column_names)          # Query in DataFrame brengen\n",
    "    \n",
    "    return df\n",
    "\n",
    "vis2_pd = vis_query(c, column_names3)\n",
    "vis2_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis2_pd['avg_salary'] = vis2_pd['avg_salary'].astype('float')\n",
    "vis2_pd['avg_homeruns'] = vis2_pd['avg_homeruns'].astype('float')\n",
    "vis2_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "vis2_np = np.array(vis2_pd)\n",
    "#numpy array verliest kolomnaam, 1ste is weight, 2de avg_salary, 3de avg_homeruns\n",
    "plt.figure()\n",
    "plt.yscale('log')\n",
    "#plt.subplot(111, projection ='3d')\n",
    "plt.scatter(vis2_np[:, 0], vis2_np[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(tight_layout=True)\n",
    "plt.yscale('log')\n",
    "bandwidth = cl.estimate_bandwidth(vis2_np, n_samples = 1000)\n",
    "clf = cl.MeanShift(bin_seeding=True)\n",
    "y = clf.fit_predict(vis2_np)\n",
    "plt.title(\"MeanShift-clustering\")\n",
    "plt.scatter(vis2_np[:, 0], vis2_np[:, 1], c=y, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "navigate_menu": false,
   "number_sections": true,
   "sideBar": true,
   "threshold": "3",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
